# サンプル: Ollama ローカル推論用プロバイダ設定
# .env 例: OLLAMA_BASE_URL=http://127.0.0.1:11434
#         ※ OLLAMA_BASE_URL が優先され、未設定の場合のみ旧互換の OLLAMA_HOST が利用されます
provider: ollama
model: llama3.1:8b-instruct
endpoint: http://127.0.0.1:11434  # 実行時は OLLAMA_BASE_URL / OLLAMA_HOST で上書きできます
seed: 0
temperature: 0.2
timeout_s: 60
