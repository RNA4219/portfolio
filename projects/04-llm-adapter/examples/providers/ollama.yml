# サンプル: Ollama ローカル推論用プロバイダ設定
# .env 例: OLLAMA_BASE_URL=http://127.0.0.1:11434
#         ※ OLLAMA_HOST は互換目的の旧変数で、設定があれば BASE_URL より優先されます
provider: ollama
model: llama3.1:8b-instruct
endpoint: http://127.0.0.1:11434  # 実行時は OLLAMA_BASE_URL / OLLAMA_HOST で上書きできます
seed: 0
temperature: 0.2
timeout_s: 60
