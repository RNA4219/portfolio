provider: local_ollama
endpoint: http://localhost:11434/api/generate
model: llama3:instruct
auth_env: null
seed: 42
temperature: 0.1
top_p: 0.9
max_tokens: 512
timeout_s: 120
retries:
  max: 1
  backoff_s: 1
persist_output: false
pricing:
  prompt_usd: 0.0
  completion_usd: 0.0
rate_limit:
  rpm: 600
  tpm: 800000
quality_gates:
  determinism_diff_rate_max: 0.2
  determinism_len_stdev_max: 12
